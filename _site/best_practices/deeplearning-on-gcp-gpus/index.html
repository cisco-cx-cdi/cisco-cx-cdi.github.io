<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Deep Learning With GPUs and Spark on Google Cloud - Cisco CX AI&amp;ML</title>
<meta name="description" content="What is Dataproc?">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Cisco CX AI&ML">
<meta property="og:title" content="Deep Learning With GPUs and Spark on Google Cloud">
<meta property="og:url" content="http://localhost:4000/best_practices/deeplearning-on-gcp-gpus/">


  <meta property="og:description" content="What is Dataproc?">







  <meta property="article:published_time" content="2018-10-01T00:00:00-07:00">



  <meta property="article:modified_time" content="2018-03-20T13:00:58-07:00">



  

  


<link rel="canonical" href="http://localhost:4000/best_practices/deeplearning-on-gcp-gpus/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Cisco CX AI&ML Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Cisco CX AI&ML</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/projects/" >Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/blog/" >Blogs</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">CDI AI&ML Team</h3>
    
    
      <p class="author__bio" itemprop="description">
        We are a group of skilled data scientists bringing actionable AI driven insights to the customer, seller, and partner domains. We consistenly bring AI&ML models to production across these three domains, and continue to push Cisco to change the way it approaches its business, and customers.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">SJC-16-4</span>
        </li>
      

      
        
          
        
          
            <li><a href="https://apps.na.collabserv.com/communities/service/html/communitystart?communityUuid=b30b6dc0-df8b-4c52-ad8d-d40ec145fe9f"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Website</a></li>
          
        
          
        
          
        
          
        
          
        
      

      

      
        <li>
          <a href="mailto:dagriff2@cisco.com">
            <meta itemprop="email" content="dagriff2@cisco.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Deep Learning With GPUs and Spark on Google Cloud">
    <meta itemprop="description" content="What is Dataproc?">
    <meta itemprop="datePublished" content="October 01, 2018">
    <meta itemprop="dateModified" content="March 20, 2018">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Deep Learning With GPUs and Spark on Google Cloud
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Deep Learning With GPUs and Spark on Google Cloud</h4></header>
              <ul class="toc__menu">
  <li><a href="#what-is-dataproc">What is Dataproc?</a></li>
  <li><a href="#what-are-the-ways-we-use-dataproc">What are the ways we use Dataproc?</a></li>
  <li><a href="#dataproc-descriptions">Dataproc Descriptions</a>
    <ul>
      <li><a href="#method-1">Method 1</a></li>
      <li><a href="#method-2">Method 2</a></li>
      <li><a href="#method-3">Method 3</a></li>
    </ul>
  </li>
  <li><a href="#what-are-the-specific-commands-we-use-to-createuse-dataproc">What are the specific commands we use to create/use Dataproc?</a>
    <ul>
      <li><a href="#method-1-from-above-section-on-ways-we-use-dataproc">Method 1 (from above section on ways we use Dataproc):</a></li>
      <li><a href="#method-2-from-above-section-on-ways-we-use-dataproc">Method 2 (from above section on ways we use Dataproc):</a></li>
      <li><a href="#method-3-from-above-section-on-ways-we-use-dataproc">Method 3 (from above section on ways we use Dataproc):</a></li>
      <li><a href="#some-other-notes">Some other notes:</a></li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <h2 id="what-is-dataproc">What is Dataproc?</h2>

<p>‘Dataproc’ is the name for a google cloud service for creating, and managing a hadoop/spark cluster in the cloud. Google cloud offers this service for automatically configuring and provisioning a set of cloud compute instances, and configures them so that they work together as a spark/hadoop cluster. The benefit of using this kind of service is that we as data scientists don’t need to focus on how to configure or manage a spark cluster. We can simply use google to create a totally managed cluster for us that we can just start using. Google documentation about dataproc can be found here: https://cloud.google.com/dataproc/docs/quickstarts.</p>

<p>The way we use Dataproc is very special. Our team aims to share the same Dataproc cluster for most of our spark jobs. There are 3 different ways in which we can do this. One is to connect to Dataproc remotely (from some other google cloud compute instance), and to run spark jobs. The other two methods require a user to ssh into the ‘master node’ of the Dataproc cluster (A cluster has one master, and many workers. The master is where spark jobs are run and managed), and running code from there. In this method, all team members must share the same master node (Which is why we call it the Fat Master Approach, because everyone shares CPUs/GPUs/RAM for non spark portions of their code). For each of these methods (which we describe below), we have special configuration scripts (which can be seen being referenced in the cluster creation commands).</p>

<h2 id="what-are-the-ways-we-use-dataproc">What are the ways we use Dataproc?</h2>

<ol>
  <li>Dataproc with ‘Datalab’ running on the master, with multiple GPUs (Fat Master Approach)</li>
  <li>Dataproc with ‘Jupyter’ running on the master, with multiple GPUs (Fat Master Approach, but with Jupyter intead of Datalab)</li>
  <li>Dataproc with ‘livy’ running on the master, where users create separate machines running ‘Datalab’ instances and submit spark jobs to the cluster remotely. (Thin Master Approach)</li>
</ol>

<h2 id="dataproc-descriptions">Dataproc Descriptions</h2>

<h3 id="method-1">Method 1</h3>

<p>In this approach, a GPU is connected to the master node of the spark cluster, and the dataproc docker container is run on the master node. Users remotely connect to the master, and then run their respective code on the master node. Thus, this is called a ‘Fat Master’ approach because the master of the spark cluster has a large amount of resources, along with attached GPUs so that Spark and Deep Learning GPU based models and frameworks can be used simultaneously. A diagram of the method is provided below.</p>

<p><img src="/assets/images/posts/datalab_method_2.png" alt="Remote Datalab" title="Remote Datalab" /></p>

<h3 id="method-2">Method 2</h3>

<p>This method is effectively the same as method 1, but instead of dataproc being run on the instance, jupyter is run instead.</p>

<h3 id="method-3">Method 3</h3>

<p>In this approach, an independent Datalab with its own connected GPU is created. This instance runs on its own, separate GCE instance outside of the Dataproc spark cluster. The Datalab instance connects to the Dataproc spark cluster master remotely by using the hosted ‘livy’ spark server (generated by an initialization script), and the ‘spark magic’ jupyter notebook package. Thus, this is called a ‘Thin Master’ approach because spark code is scheduled and run on the remote Dataproc cluster, while any GPU and local python code is run in the Datalab instance. A diagram of the method is provided below.</p>

<p><img src="/assets/images/posts/datalab_method_1.png" alt="Datalab Remote Spark" title="Datalab Remote Spark" /></p>

<h2 id="what-are-the-specific-commands-we-use-to-createuse-dataproc">What are the specific commands we use to create/use Dataproc?</h2>

<h3 id="method-1-from-above-section-on-ways-we-use-dataproc">Method 1 (from above section on ways we use Dataproc):</h3>

<p><strong>Custom script steps for creating a dataproc/datalab cluster with attached GPUs:</strong></p>
<ol>
  <li>Use a modified google ‘datalab’ for dataproc initialization script.</li>
  <li>Install Nvidia drivers on node.</li>
  <li>Install nvidia-docker and modfiy initializaiton script to use the ‘–runtime=nvidia’ option.</li>
</ol>

<p><strong>Steps to Create And Connect to the Cluster:</strong></p>

<ol>
  <li>Command to create instance:
Note here that we have custom initialization scripts for installing GPU drivers in the datalab GCE instance, installing nvidia-docker, and running the datalab docker image in nvidia-docker.</li>
</ol>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud beta dataproc clusters create spark-cluster \
--bucket vm_space \
--subnet default \
--zone us-central1-a \
--master-machine-type n1-highmem-8 \
--master-boot-disk-size 50 \
--master-min-cpu-platform "Intel Skylake" \
--worker-min-cpu-platform "Intel Skylake" \
--num-workers 2 \
--num-preemptible-workers 4 \
--worker-machine-type n1-highmem-64 \
--worker-boot-disk-size 200 \
--preemptible-worker-boot-disk-size 200 \
--image-version 1.3 \
--project gvs-cs-cisco \
--metadata CONDA_PACKAGES="python==3.5 nltk numpy scikit-learn keras",MY_PY2_PACKAGES="tensorflow-gpu==1.8.0 keras",MY_PY3_PACKAGES="tensorflow-gpu==1.8.0 keras nltk gensim scikit-learn" \
--initialization-actions 'gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://dataproc-initialization-actions/livy/livy.sh,gs://dataproc-initialization-actions/conda/install-conda-env.sh,gs://vm_init_scripts/hseokho-datalab-gpu.sh,gs://vm_init_scripts/hseokho-configs.sh' \
--scopes cloud-platform \
--master-accelerator type=nvidia-tesla-v100 \
--properties "\
yarn:yarn.scheduler.minimum-allocation-vcores=4,\
capacity-scheduler:yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DominantResourceCalculator,\
spark:spark.dynamicAllocation.enabled=true,\
spark:spark.dynamicAllocation.executorIdleTimeout=5m,\
spark:spark.dynamicAllocation.cachedExecutorIdleTimeout=1h,\
spark:spark.dynamicAllocation.initExecutors=10,\
spark:spark.dynamicAllocation.maxExecutors=500,\
spark:spark.dynamicAllocation.minExecutors=1,\
spark:spark.dynamicAllocation.schedulerBacklogTimeout=1s,\
spark:spark.driver.memory=40g,\
spark:spark.executor.instances=80,\
spark:spark.executor.memory=18g,\
spark:spark.executor.cores=4,\
spark:spark.task.maxFailures=4,\
spark:spark.driver.maxResultSize=8g"
</code></pre></div></div>

<ol>
  <li>Connect to the instance with an ssh tunnel and ‘SOCKS’ proxy server:</li>
</ol>

<p>(In one terminal window, run)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud compute ssh --ssh-flag="-D" --ssh-flag="10000" --zone="us-central1-a" "spark-cluster-m"
</code></pre></div></div>

<ol>
  <li>Connect to the datalab instance (running in a docker instance on the dataproc master node) thorough your local browser (This command is for MacOs). The basic idea is that in step 2 you created an ssh connection to your cluster, and started a socks proxy locally. This command opens your web browser and tells it to route all webpage requests to the socks server running locally, and pipe them through the ssh terminal to the master node:
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome" "http://spark-cluster-m:8080" \
--proxy-server="socks5://127.0.0.1:10000" \
--host-resolver-rules="MAP * 0.0.0.0 , EXCLUDE localhost" \
--user-data-dir=/tmp/spark-cluster-m
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="method-2-from-above-section-on-ways-we-use-dataproc">Method 2 (from above section on ways we use Dataproc):</h3>

<p>Note that most of this process requires the creator to ssh into the instance, and run many commands themselves (commands that are already bundled together in the dataproc initialization scripts for Method 1).</p>

<ol>
  <li>Create the instance through the google cloud console, or the command line:
(With GPU attached to MASTER only)
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud beta dataproc clusters create spark-jupyter-cluster \
--bucket vm_space \
--subnet default \
--zone us-central1-a \
--master-machine-type n1-standard-4 \
--master-boot-disk-size 500 \
--num-workers 2 \
--worker-machine-type n1-standard-4 \
--worker-boot-disk-size 500 \
--image-version 1.2 \
--project gvs-cs-cisco \
--initialization-actions 'gs://dan-test-bucket/notebooks/create-my-cluster.sh,gs://dataproc-initialization-actions/jupyter/jupyter.sh' \
--master-accelerator type=nvidia-tesla-k80
</code></pre></div>    </div>
  </li>
  <li>SSH into the instance, and run/install the NVIDIA GPU drivers. (install_gpu_drivers.sh):
(Run from the command line on the instance to install nvidia drivers, cuda, cudnn)</li>
</ol>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># INSTALL NVIDIA DRIVERS
# Note: The driver install script, CUDA install script, and cuDNN library .tgz file
# were all directly downloaded from the Nvidia website, and were specified as
# being for K-80 GPUs, on the 'Generic Linux 64 bit' operating system. More detailed
# Instructions for how to install the following packages can be found at
# These links: (Cuda install: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html)
# (CuDNN install: https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html)

sudo apt-get update
sudo apt-get install -y linux-headers-$(uname -r)
gsutil cp gs://dan-test-bucket/notebooks/NVIDIA-Linux-x86_64-384.145.run .
chmod a+rwx NVIDIA-Linux-x86_64-384.145.run
sudo ./NVIDIA-Linux-x86_64-384.145.run -a -s

# INSTALL CUDA 9.0
gsutil cp gs://dan-test-bucket/notebooks/cuda_9.0.176_384.81_linux.run .
chmod a+rwx cuda_9.0.176_384.81_linux.run
sudo ./cuda_9.0.176_384.81_linux.run --silent
export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}

# INSTALL cuDNN 7.3
gsutil cp gs://dan-test-bucket/notebooks/cudnn-9.0-linux-x64-v7.3.1.20.tgz .
tar -xzvf cudnn-9.0-linux-x64-v7.3.1.20.tgz
sudo cp cuda/include/cudnn.h /usr/local/cuda/include
sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
</code></pre></div></div>

<p>3) Start a SOCKS server locally, that routes commands to the instance:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud compute ssh --ssh-flag="-D" --ssh-flag="10000" --zone="us-central1-a" "spark-jupyter-cluster-m
</code></pre></div></div>

<p>4) Open google chrome, and direct it to use the locally running socks proxy. (I’ve included two commands because it depends on the port you are running your jupyter notebook off of. It may not be 8080 for jupyter as it is for dataproc):
(For Mac)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome" "http://spark-jupyter-cluster:8123" \
--proxy-server="socks5://localhost:10000" \
--host-resolver-rules="MAP * 0.0.0.0 , EXCLUDE localhost" \
--user-data-dir=/tmp/ \
--incognito
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome" "http://spark-jupyter-cluster:8080" \
--proxy-server="socks5://localhost:10000" \
--host-resolver-rules="MAP * 0.0.0.0 , EXCLUDE localhost" \
--user-data-dir=/tmp/ \
--incognito
</code></pre></div></div>

<h3 id="method-3-from-above-section-on-ways-we-use-dataproc">Method 3 (from above section on ways we use Dataproc):</h3>

<ol>
  <li>Create the DATAPROC cluster with ‘livy’ installed so spark jobs (Note, this is just the cluster):
(NOTE: You must ALLOW http/https traffic to the master node)</li>
</ol>

<p>MoreNotes:</p>
<ul>
  <li>https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Magics%20in%20IPython%20Kernel.ipynb</li>
  <li>https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Pyspark%20Kernel.ipynb</li>
  <li>https://github.com/jupyter-incubator/sparkmagic</li>
  <li>https://github.com/GoogleCloudPlatform/dataproc-initialization-actions/tree/master/livy</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud dataproc clusters create spark-livy-cluster \
--bucket dan-test-bucket \
--subnet default \
--zone us-central1-a \
--master-machine-type n1-standard-4 \
--master-boot-disk-size 500 \
--num-workers 2 \
--worker-machine-type n1-standard-4 \
--worker-boot-disk-size 500 \
--image-version 1.2 \
--project gvs-cs-cisco \
--initialization-actions gs://dataproc-initialization-actions/livy/livy.sh \
--metadata 'MINICONDA_VERSION=latest' \
--scopes cloud-platform \
--properties "\
dataproc:alpha.autoscaling.enabled=true,\
dataproc:alpha.autoscaling.primary.max_workers=100,\
dataproc:alpha.autoscaling.secondary.max_workers=100,\
dataproc:alpha.autoscaling.cooldown_period=10m,\
dataproc:alpha.autoscaling.scale_up.factor=0.05,\
dataproc:alpha.autoscaling.graceful_decommission_timeout=1h"
</code></pre></div></div>

<ol>
  <li>Create a separate GPU enabled DATALAB instance (Note this is a separate machine from the cluster):
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>datalab --zone us-central1-a beta create-gpu my-datalab \
--no-backups \
--accelerator-count 1 \
--machine-type n1-standard-4 \
--idle-timeout 1d \
--no-backups \
--network-name default
</code></pre></div>    </div>
  </li>
  <li>Connect to the datalab instance, and open the instance in your local browser at ‘http://localhost:8081’:
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>datalab connect my-datalab --no-user-checking
</code></pre></div>    </div>
  </li>
</ol>
<p>&lt;Then open your local web browser to http://localhost:8081&gt;</p>

<ol>
  <li>SSH into the datalab instance <Optional>:
```
gcloud compute --project gvs-cs-cisco ssh --zone us-central1-a my-datalab</Optional></li>
</ol>

<h1 id="this-lists-the-running-docker-containers">This lists the running docker containers</h1>
<p>docker ps
docker exec -it <container> bash</container></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
4. Connect to the running docker instance and install keras and 'sparkmagic':
 * pip install keras
 * conda install sparkmagic
 * If you want to install from the jupyter notebook, run the following
</code></pre></div></div>
<h1 id="install-a-conda-package-in-the-current-jupyter-kernel">Install a conda package in the current Jupyter kernel</h1>
<p>import sys
!conda install –yes –prefix {sys.prefix} sparkmagic</p>

<h1 id="install-a-pip-package-in-the-current-jupyter-kernel">Install a pip package in the current Jupyter kernel</h1>
<p>import sys
!{sys.executable} -m pip install keras
```</p>

<ol>
  <li>Setup the connection to the spark dataproc cluster:
    <ul>
      <li>Enable http/https on the dataproc cluster</li>
      <li>Run the ‘sparkmagic’ spark manager.</li>
      <li>Create a new endpoint ‘http://spark-livy-cluster-m:8998’ with ‘No auth’</li>
      <li>Create a new session with the specified endpoint</li>
      <li>Add ‘%%spark’ to any spark related code you want run on the cluster.</li>
    </ul>
  </li>
</ol>

<h3 id="some-other-notes">Some other notes:</h3>

<p><strong>Some important notes to share:</strong></p>

<ul>
  <li>GCP does not actually have weaker cores than our Hadoop. With this config, GCP outperforms our Hadoop per-core.  Two factors contributing to the previously incorrect benchmarking:
    <ul>
      <li>Having many smaller nodes is substantially slower than having a few big nodes. This should be because there’s less shuffling over the network and more shuffling in memory (haven’t verified this) but I think it’s actually because we get entire sets of 20+ core CPUs rather than partial CPUs. I don’t really know, if someone can test and better understand this, I’d be much obliged (I’m curious).</li>
      <li>You can specify which generation of Intel Xeon processors we get… for free. The config below specifies Skylake, which is the best possible, but you can’t set it for preemptible nodes so they’re Sandy Bridges. I don’t think this massively impacts performance though.</li>
    </ul>
  </li>
  <li>Please add pre-emptible nodes instead of worker nodes to increase cluster size: preemptibles are 1/4th the cost and has very little downside for what we do.</li>
  <li>GCP’s default docker initialization script has a bug (dataproc-initialization-actions/docker/docker.sh) which I’ve fixed in gs://vm_init_scripts/hseokho-docker.sh. This was causing nodes to fail to start, and prevented YARN from recognizing new nodes added to the cluster.</li>
  <li>Executor configs are 18gb/exec, 4cores/exec. This is the most similar config per-exec to the defaults (jupyter100, jupyter200) on Hadoop. To get jupyter100mem-equivalent specs, you need to use ultra memory nodes in addition to adjusting these params</li>
  <li>Driver memory is 40gb, but DataLab starts to get unstable around ~10GB memory used.</li>
  <li>GPUs need to be shared (currently 1 v100). I think cluster needs to be restarted to attach more. I recommend that for GPU-heavy tasks you just make your own cluster for now.</li>
</ul>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/#google-cloud" class="page__taxonomy-item" rel="tag">Google Cloud</a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#best-practices" class="page__taxonomy-item" rel="tag">Best_Practices</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-03-20">March 20, 2018</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Deep+Learning+With+GPUs+and+Spark+on+Google+Cloud%20http%3A%2F%2Flocalhost%3A4000%2Fbest_practices%2Fdeeplearning-on-gcp-gpus%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fbest_practices%2Fdeeplearning-on-gcp-gpus%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2Fbest_practices%2Fdeeplearning-on-gcp-gpus%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fbest_practices%2Fdeeplearning-on-gcp-gpus%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/conference/google-cloud-conference-2018/" class="pagination--pager" title="Google Cloud 2018 Conference Highlights
">Previous</a>
    
    
      <a href="/best_practices/deeplearning-gcp-mlengine-datalab-compare/" class="pagination--pager" title="Deep Learning on GCP: Datalab vs ML Engine
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/best_practices/calibration-curves-ml/" rel="permalink">Calibration Curves: How Confident Are Your Confidence Scores?
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">##

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/conference/open-data-science-conference/" rel="permalink">Open Data Science 2018 Conference Highlights
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Conference Date: 11/2/2018-11/3/2018

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/best_practices/deeplearning-gcp-mlengine-datalab-compare/" rel="permalink">Deep Learning on GCP: Datalab vs ML Engine
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">What is ML Engine?

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/conference/google-cloud-conference-2018/" rel="permalink">Google Cloud 2018 Conference Highlights
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Conference Date: 07/24/2018-07/26/2018

</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://apps.na.collabserv.com/communities/service/html/communitystart?communityUuid=b30b6dc0-df8b-4c52-ad8d-d40ec145fe9f"><i class="fas fa-link" aria-hidden="true"></i> Cisco Communities Page</a></li>
        
      
        
          <li><a href="https://wwwin-github.cisco.com/GVS-CS-DSX"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Cisco CX AI&ML. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>








  </body>
</html>
